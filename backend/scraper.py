#!/usr/bin/env python3
"""
LCSC ç”µå­å…ƒå™¨ä»¶çˆ¬è™«
å®šæ—¶ä» LCSC æŠ“å–å™¨ä»¶æ•°æ®ï¼Œæä¾›å®æ—¶ä»·æ ¼å’Œåº“å­˜æŸ¥è¯¢
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
import logging
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import quote
from functools import wraps

# é…ç½®
BASE_URL = "https://www.lcsc.com"
SEARCH_URL = f"{BASE_URL}/search"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
}

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds

# çƒ­é—¨æœç´¢åˆ—è¡¨
POPULAR_PARTS = [
    "STM32F103C8T6",
    "GD32F103C8T6",
    "ESP32-WROOM-32",
    "ESP32-C3FH4",
    "CH340N",
    "CH340G",
    "LD1117V33",
    "AMS1117-3.3",
    "LM358",
    "SGM358",
    "AO3400",
    "2N7000",
    "IRF540N",
    "NE555",
    "ATMEGA328P",
    "74HC595",
    "74HC04",
    "MAX232",
    "LM317",
    "AMS1117",
]


# é‡è¯•è£…é¥°å™¨
def retry_on_failure(max_retries: int = MAX_RETRIES, delay: int = RETRY_DELAY):
    """è¯·æ±‚å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•çš„è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(1, max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    logger.warning(f"ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt < max_retries:
                        time.sleep(delay)
            logger.error(f"é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥")
            raise last_exception
        return wrapper
    return decorator


def safe_find(element: BeautifulSoup, selectors: List[tuple]) -> Optional[Any]:
    """
    å°è¯•å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾å…ƒç´ 
    
    Args:
        element: BeautifulSoup å…ƒç´ 
        selectors: é€‰æ‹©å™¨åˆ—è¡¨ [(tag, class_name), ...]
    
    Returns:
        æ‰¾åˆ°çš„å…ƒç´ æˆ– None
    """
    for tag, cls in selectors:
        result = element.find(tag, class_=re.compile(cls, re.I))
        if result:
            return result
    return element.find('a') or element.find()


def get_text_safe(element: Optional[Any], default: str = "") -> str:
    """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
    if element:
        return element.get_text(strip=True)
    return default


@retry_on_failure(max_retries=3, delay=2)
def fetch_url(url: str, timeout: int = 30) -> Optional[str]:
    """
    è·å– URL å†…å®¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        url: ç›®æ ‡ URL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        å“åº”æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
    """
    logger.info(f"Fetching: {url}")
    response = requests.get(url, headers=HEADERS, timeout=timeout)
    response.raise_for_status()
    return response.text


def search_lcsc(keyword: str) -> List[Dict[str, Any]]:
    """
    æœç´¢ LCSC è·å–å™¨ä»¶ä¿¡æ¯
    
    Args:
        keyword: æœç´¢å…³é”®è¯
    
    Returns:
        å™¨ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    try:
        encoded_keyword = quote(keyword, safe='')
        url = f"{SEARCH_URL}?q={encoded_keyword}"
        
        logger.info(f"ğŸ” æ­£åœ¨æœç´¢: {keyword}...")
        print(f"ğŸ” æ­£åœ¨æœç´¢: {keyword}...")
        
        # ä½¿ç”¨å¸¦é‡è¯•çš„ fetch
        html_content = fetch_url(url, timeout=30)
        if not html_content:
            logger.error(f"è·å–æœç´¢ç»“æœå¤±è´¥: {keyword}")
            return []
        
        # è§£ææœç´¢ç»“æœ
        soup = BeautifulSoup(html_content, 'lxml')
        results = []
        
        # æŸ¥æ‰¾äº§å“åˆ—è¡¨ - LCSC çš„ç»“æ„å¯èƒ½å˜åŒ–
        product_cards = soup.find_all('div', class_='search-product-list') or \
                       soup.find_all('div', class_='product-list') or \
                       soup.find_all('div', class_='goods-list')
        
        if not product_cards:
            # å¤‡é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„äº§å“å®¹å™¨
            product_cards = soup.find_all('div', class_=re.compile(r'product|goods|item'))
        
        for card in product_cards[:10]:  # æœ€å¤šå– 10 ä¸ªç»“æœ
            try:
                part_info = extract_product_info(card)
                if part_info:
                    results.append(part_info)
            except Exception as e:
                logger.debug(f"è§£æäº§å“å¡ç‰‡å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        return results
        
    except Exception as e:
        logger.error(f"âŒ æœç´¢å‡ºé”™: {e}")
        print(f"âŒ æœç´¢å‡ºé”™: {e}")
        return []


def extract_product_info(card) -> Optional[Dict[str, Any]]:
    """
    ä»äº§å“å¡ç‰‡ä¸­æå–ä¿¡æ¯
    
    Args:
        card: BeautifulSoup äº§å“å¡ç‰‡å…ƒç´ 
    
    Returns:
        å™¨ä»¶ä¿¡æ¯å­—å…¸ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [
            ('a', 'product-name'),
            ('a', 'goods-name'),
            ('a', 'name'),
            ('a', 'product-title'),
        ]
        
        name_elem = safe_find(card, selectors)
        
        if not name_elem:
            # æŸ¥æ‰¾ä»»ä½•é“¾æ¥
            name_elem = card.find('a')
        
        name = get_text_safe(name_elem)
        
        # æå–ä»·æ ¼
        price_elem = card.find(class_=re.compile(r'price|current-price'))
        price = get_text_safe(price_elem)
        
        # æå–åº“å­˜
        stock_elem = card.find(class_=re.compile(r'stock|inventory'))
        stock = get_text_safe(stock_elem)
        
        # æå–å‹å·
        part_elem = card.find(class_=re.compile(r'model|part-number'))
        part = get_text_safe(part_elem)
        
        if not part and name:
            part = name.split()[0] if name.split() else name
        
        # æå–é“¾æ¥
        link = ""
        if name_elem and name_elem.get('href'):
            link = name_elem['href']
            if not link.startswith('http'):
                link = BASE_URL + link
        
        return {
            "part": part or "Unknown",
            "name": name or "",
            "price": price,
            "stock": stock,
            "link": link,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"æå–äº§å“ä¿¡æ¯å¤±è´¥: {e}")
        return None


@retry_on_failure(max_retries=2, delay=1)
def get_product_detail(url: str) -> Optional[Dict[str, Any]]:
    """
    è·å–äº§å“è¯¦æƒ…é¡µ
    
    Args:
        url: äº§å“è¯¦æƒ…é¡µ URL
    
    Returns:
        è¯¦æƒ…ä¿¡æ¯å­—å…¸
    """
    try:
        html_content = fetch_url(url, timeout=30)
        if not html_content:
            return None
        
        soup = BeautifulSoup(html_content, 'lxml')
        
        # æå–è¯¦æƒ…
        data = {}
        
        # åˆ¶é€ å•†
        mfr_elem = soup.find(class_=re.compile(r'manufacturer|mfr'))
        if mfr_elem:
            data['manufacturer'] = get_text_safe(mfr_elem)
        
        # æè¿°
        desc_elem = soup.find(class_=re.compile(r'description|detail-desc'))
        if desc_elem:
            data['description'] = get_text_safe(desc_elem)[:200]
        
        # å°è£…
        pkg_elem = soup.find(class_=re.compile(r'package|footprint'))
        if pkg_elem:
            data['package'] = get_text_safe(pkg_elem)
        
        # ä»·æ ¼åŒºé—´
        price_table = soup.find('table', class_=re.compile(r'price-table'))
        if price_table:
            prices = []
            for row in price_table.find_all('tr'):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    qty = get_text_safe(cells[0])
                    price = get_text_safe(cells[1])
                    prices.append({"qty": qty, "price": price})
            data['pricing'] = prices
        
        logger.info(f"âœ… è·å–è¯¦æƒ…æˆåŠŸ: {url}")
        return data
        
    except Exception as e:
        logger.error(f"âŒ è·å–è¯¦æƒ…å¤±è´¥: {e}")
        print(f"âŒ è·å–è¯¦æƒ…å¤±è´¥: {e}")
        return None


def scrape_all_popular() -> Dict[str, Any]:
    """
    çˆ¬å–æ‰€æœ‰çƒ­é—¨å™¨ä»¶
    
    Returns:
        åŒ…å«æ‰€æœ‰å™¨ä»¶æ•°æ®çš„å­—å…¸
    """
    print("=" * 60)
    print("ğŸ¤– LCSC ç”µå­å…ƒå™¨ä»¶æ•°æ®çˆ¬è™«")
    print("=" * 60)
    start_time = datetime.now()
    print(f"â° å¼€å§‹æ—¶é—´: {start_time.isoformat()}")
    print(f"ğŸ“¦ å°†çˆ¬å– {len(POPULAR_PARTS)} ä¸ªçƒ­é—¨å™¨ä»¶")
    print("=" * 60)
    
    all_data = {
        "meta": {
            "updated": start_time.isoformat(),
            "source": "LCSC (ç«‹åˆ›å•†åŸ)",
            "version": "1.1.0",
            "part_count": len(POPULAR_PARTS)
        },
        "parts": []
    }
    
    for i, part in enumerate(POPULAR_PARTS, 1):
        print(f"\n[{i}/{len(POPULAR_PARTS)}] ", end="")
        
        # æœç´¢è·å–åŸºæœ¬ä¿¡æ¯
        results = search_lcsc(part)
        
        if results:
            # å–ç¬¬ä¸€ä¸ªç»“æœä½œä¸ºä¸»è¦ä¿¡æ¯
            main_result = results[0]
            
            # å¦‚æœæœ‰è¯¦æƒ…é¡µé“¾æ¥ï¼Œè·å–æ›´å¤šä¿¡æ¯
            if main_result.get('link'):
                detail = get_product_detail(main_result['link'])
                if detail:
                    main_result.update(detail)
            
            all_data['parts'].append(main_result)
        else:
            # æœç´¢å¤±è´¥ï¼Œæ·»åŠ å ä½æ•°æ®
            all_data['parts'].append({
                "part": part,
                "name": part,
                "price": "æŸ¥è¯¢ä¸­",
                "stock": "æŸ¥è¯¢ä¸­",
                "link": f"{SEARCH_URL}?q={quote(part)}",
                "timestamp": start_time.isoformat(),
                "status": "not_found"
            })
        
        # ç¤¼è²Œæ€§å»¶è¿Ÿ
        time.sleep(1)
    
    end_time = datetime.now()
    print("\n" + "=" * 60)
    print(f"âœ… çˆ¬å–å®Œæˆ! å…±è·å– {len(all_data['parts'])} ä¸ªå™¨ä»¶")
    print(f"â° ç»“æŸæ—¶é—´: {end_time.isoformat()}")
    print(f"â±ï¸ è€—æ—¶: {(end_time - start_time).total_seconds():.1f} ç§’")
    print("=" * 60)
    
    return all_data


def save_to_json(data: Dict[str, Any], filename: str = "parts.json") -> None:
    """
    ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶
    
    Args:
        data: è¦ä¿å­˜çš„æ•°æ®
        filename: æ–‡ä»¶å
    """
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")


def main() -> None:
    """
    ä¸»å‡½æ•°
    """
    # çˆ¬å–æ•°æ®
    data = scrape_all_popular()
    
    # ä¿å­˜æ•°æ®
    save_to_json(data, "../data/parts.json")
    
    # åŒæ—¶ä¿å­˜ä¸€ä»½å¸¦æ—¶é—´æˆ³çš„ç‰ˆæœ¬
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_to_json(data, f"../data/parts_{timestamp}.json")
    
    print("\nğŸ“‹ çˆ¬å–æ‘˜è¦:")
    for part in data['parts']:
        status = "âœ…" if part.get('status') != 'not_found' else "âš ï¸"
        print(f"  {status} {part['part']}: {part['price']} | {part['stock']}")


if __name__ == "__main__":
    main()
